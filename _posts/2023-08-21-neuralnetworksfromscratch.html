---
layout: post
title: "Neural Networks from Scratch"
subtitle: "ML: Neural Networks from Scratch i python tutorial"
date: 2023-09-5 4:45:13 -0400
background: '/img/posts/Screenshot.png'
---


<p>Jeg lærte igen om vægte og bias. Hvert neuron har for hvert input den modtager en vægt. Hvert indput bliver ganget med dens vægt. Hvert neuron har et bias, som bliver lagt til produktet af inputs og vægtene. Inputs kan både være inputs fra start laget, eller outputs fra andre neuroner fra et gemt lag “hidden layer”. Hvis det er inputs fra start laget kunne det fx være værdier fra en lækage test. Hvert unikt input kunne for eksempel være data fra en bestemt sensor.</p>
<img src="/img/posts/neuronweightbias.png" style="max-width: 100%"> 
<p>Der bliver introduceret konceptet “Shape”. Hvis man har en liste i python, så afhænger dens shape af hvor mange elementer den har, så hvis der er fire elementer, så er har den en shape på 4.  Hvis det er en liste med to lister i, som hver har fire elementer så er dens shape(2,4). De skal være ens, ellers ville man ikke kunne afgøre deres shape. En 2d array er også en matrix, en matrix er et rektangel formet array. Man bruger prik prodktet til at gange inputs med weights. I python kan man bruge nympy til dette(import numpy as np), vha. np.dot(inputs, weights).</p> 
<img src="/img/posts/dotproduct.png" style="max-width: 100%">
<p>Prik produktet fåes ved at gange elementer fra hvert index og lægge dem sammen. Når man ganger to matrices sammen så ganger man hver række med hver kolonne, så man får et nyt matrix med prik produktet. For at man kan gange inputs og weights sammen for hvert element, skal der være lige mange i rækker og kolonnerne. Man kan skife rækker og kolonner på et matrix for at få det til at passe, dette kaldes for at transpose (np.dot(inputs, np.array(weights).T)). Input data’en bliver gemt i en “batch”, hvilket også er et 2d array/matrix, man bruger et batch af inputs af gangen istedet for kun en række af inputs af gangen fordi det er mere effektivt. Vægtene er i starten random mellem -1 og 1.</p>
<img src="/img/posts/matrixdot1.png" style="max-width: 100%">
<br><br>
<img src="/img/posts/matrixdot3.png" style="max-width: 100%"> 
<p>Aktiverings funktioner: Hvert neuron har en aktiverings funktion, efter inputs er ganget med vægte og plusset med bias, passerer den en aktiverings funktion, som der afgør outputtet, “hidden layers” og det sidste output lag har ofte forskellige aktiverings funktioner. Det findes flere forskellige aktiverings funktioner: step-, relu-, softmax- og sigmoid- funktioner(har gennemgået sigmoid ved en anden post). Step er blot hvor outputtet enten er 0 eller 1 afhængigt af værdien, hvis værdien passerer 0 bliver den 1.</p> 
<img src="/img/posts/stepfunction.png" style="max-width: 100%"> 
<p>Ved relu funktionen, hvis værdien passerer 0, er outputtet lig med den samme værdi som der kommer ind funktionen, ellers er den nul.</p> 
<img src="/img/posts/relu.png" style="max-width: 100%"> 
<p>Relu(skrives ReLU) funktionen er den mest anvendte i gemte lag(hidden layers).  Hvis man ikke bruger en aktiverings funktion, bliver vægte gange inputs plus bias bare en liniær funktion. Dette er ikke så brugbart til at løse problemer der ikke er liniære i deres natur. Ved det sidste lag bruger man en softmax aktiverings funktion, istedet for ReLU. Man tager hvert output og tager det som en eksponent til e, som en exponentiel funktion, med eulers nummer (ca 2.7182…). Før man gør dette laver man “overflow prevention”, dette er en sikkerhedsforanstaltning hvis et af output numrene er et stort tal, fordi når man tager dette nummer som eksponent kan det nemt blive for meget stort og man kan få en fejl ved det. Man undgår dette ved at minusse det største tal i outputtet med hvert nummer, derefter kan man tage det som eksponent til e. Efter overflow prevention nomaliserer man de numre man får fra den eksponentielle funktion, ved at dividere hvert nummer med summen af numrene fra outputs. Så softmax funktionen er at eksponere output numrene fra det sidste lag og derefter normalisere dem.</p> 
<img src="/img/posts/softmax.png" style="max-width: 100%"> 

<img src="/img/posts/softmaxwithoverflowprevention.png" style="max-width: 100%"> 
<p>Efter at du har anvendt softmax funktionen på outputs, får du en fordeling af sandsynlighed over dine klasser (eller hvad end output numrene repræsenterer). Dette gør det nemt at vælge den mest sandsynlige klasse ved at tage det højeste output, eller at få overblik over usikkerhed ved at se på fordelingen af sandsynlighederne. Det skal siges at “Neural Networks from Scratch” bruger klassifikation med neurale netværk som lærings eksempel. Aktiveringsfunktionen afhænger ofte af den specifikke opgave og den ønskede outputform. Afhængigt af den specifikke opgave kan forskellige aktiveringsfunktioner være mere hensigtsmæssige. Mens softmax ofte bruges til fler-klasses klassifikation, kan andre funktioner som fx sigmoid være mere passende for binær klassifikation og ved regression kan man lade være med at bruge en aktiveringsfunktion dvs. en lineær funktion fra input gange vægt plus bias y = wx+b. </p>
<p>Der bliver også forklaret om loss-functions, i dette tilfælde “Categorical Cross-Entropy”, hvilket er en populær funktion til klassifikation. Kort fortalt bliver formlen til: -log(prediction), hvor prediction er den klasse der bliver forudsagt i forhold til den sande klasse, i hvert tilfælde. Så jo sikre forudsigelsen er jo mindre er loss, og omvendt jo mindre tallet for sikkerheden af forudsigelsen er, jo højere er loss. Fx -log(0.7) = 0.36 og -log(0.5) = 0.69. I hver række i batchen  </p>
